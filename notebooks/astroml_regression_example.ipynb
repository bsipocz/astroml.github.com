{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurement Errors in Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets use simulated data here. \n",
    "First we will model the distance of 100 supernovas (for a particular cosmology) as a function of redshift.\n",
    "\n",
    "We rely on that astroML has a common API with scikit-learn, extending the functionality of the latter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: remove this when the release is out, and astroML 1.0 is installed\n",
    "!pip install -U https://github.com/bsipocz/astroML/archive/regression_with_errors.zip arviz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.cosmology import LambdaCDM\n",
    "from astroML.datasets import generate_mu_z\n",
    "\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0)\n",
    "\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.30, Ode0=0.70, Tcmb0=0)\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu_true = cosmo.distmod(z)\n",
    "\n",
    "plt.errorbar(z_sample, mu_sample, dmu, fmt='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regression\n",
    "\n",
    "Regression defined as the relation between a dependent variable, $y$, and a set of independent variables, $x$, \n",
    "that describes the expectation value of y given x: $ E[y|x] $\n",
    "\n",
    "We will start with the most familiar linear regression, a straight-line fit to data.\n",
    "A straight-line fit is a model of the form\n",
    "$$\n",
    "y = ax + b\n",
    "$$\n",
    "where $a$ is commonly known as the *slope*, and $b$ is commonly known as the *intercept*.\n",
    "\n",
    "We can use Scikit-Learn's LinearRegression estimator to fit this data and construct the best-fit line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LinearRegression_sk \n",
    "\n",
    "linear_sk = LinearRegression_sk()\n",
    "linear_sk.fit(z_sample[:,None], mu_sample)\n",
    "\n",
    "mu_fit_sk = linear_sk.predict(z[:, None])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_sk, '-k')\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement errors\n",
    "\n",
    "Modifications to LinearRegression in astroML take measurement errors into account on the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit = linear.predict(z[:, None])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_sk, '-k')\n",
    "ax.plot(z, mu_fit, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis function regression\n",
    "\n",
    "If we consider a function in terms of the sum of bases (this can be polynomials, Gaussians, quadratics, cubics) then we can solve for the coefficients using regression. \n",
    "\n",
    "### Polynomial basis functions\n",
    "\n",
    "polynomial regression: $$ùë¶=ùëé_0+ùëé_1ùë•+ùëé_2ùë•^2+ùëé_3ùë•^3+‚ãØ$$\n",
    "\n",
    "Notice that this is still a linear model‚Äîthe linearity refers to the fact that the coefficients $ùëé_ùëõ$ never multiply or divide each other. What we have effectively done here is to take our one-dimensional $ùë•$ values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between $ùë•$ and $ùë¶$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import PolynomialRegression\n",
    "\n",
    "# 2nd degree polynomial regression\n",
    "polynomial = PolynomialRegression(degree=2)\n",
    "polynomial.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit_poly = polynomial.predict(z[:, None])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_poly, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian basis functions\n",
    "\n",
    "Of course, other basis functions are possible.\n",
    "For example, one useful pattern is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian bases. E.g. we could substitute $ùë•^2$ for Gaussians (where we fix $ùúé$ and $ùúá$ and fit for the amplitude) as long as the attribute we are fitting for is linear. This is called basis function regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import BasisFunctionRegression\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our number of Gaussians\n",
    "nGaussians = 20\n",
    "basis_mu = np.linspace(0, 2, nGaussians)[:, None]\n",
    "basis_sigma = 3 * (basis_mu[1] - basis_mu[0])\n",
    "\n",
    "gauss_basis = BasisFunctionRegression('gaussian', mu=basis_mu, sigma=basis_sigma)\n",
    "gauss_basis.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit_gauss = gauss_basis.predict(z[:, None])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_gauss, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and cross-validation\n",
    "\n",
    "As the complexity of a model increases the data points fit the model more and more closely.\n",
    "\n",
    "This does not result in a better fit to the data. We are overfitting the data (the model has high variance - a small change in a training point can change the model dramatically).\n",
    "\n",
    "We can evaluate this using a training set (50-70% of sample), a cross-validation set (15-25%) and a test set (15-25%). See booko sub-chapter 8.11 and e.g. figures [8.13](http://www.astroml.org/book_figures/chapter8/fig_cross_val_B.html) and [8.14](http://www.astroml.org/book_figures/chapter8/fig_cross_val_C.html).\n",
    "\n",
    "For cases where we are concerned with overfitting we can apply constraints (usually of smoothness, number of coefficients, size of coefficients). See book sub-chapter 8.3, and e.g. figure [8.4](http://www.astroml.org/book_figures/chapter8/fig_rbf_ridge_mu_z.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurement errors in both dependent and independent variables\n",
    "\n",
    "Use simulation data from [Kelly 2007](https://iopscience.iop.org/article/10.1086/519947/pdf) where there is measurement error on the observed values $x_i$ and $y_i$ as well as intrinsic scatter in the regression relationship: $$ \\eta_i = \\alpha + \\beta \\xi_i + \\epsilon_i $$ and $$ x_i = \\xi_i + \\epsilon_{x,i}$$ $$y_i = \\eta_i + \\epsilon_{y,i}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.datasets import simulation_kelly\n",
    "\n",
    "ksi, eta, xi, yi, xi_error, yi_error, alpha_in, beta_in = simulation_kelly(size=100, scalex=0.2, scaley=0.2,\n",
    "                                                                           alpha=2, beta=1,\n",
    "                                                                           multidim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegressionwithErrors will be part of the next astroML release, v1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import LinearRegressionwithErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linreg_xy_err = LinearRegressionwithErrors()\n",
    "linreg_xy_err.fit(xi, yi, yi_error, xi_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some plotting functions we use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure(ksi, eta, x, y, sigma_x, sigma_y, add_regression_lines=False,\n",
    "                alpha_in=1, beta_in=0.5):\n",
    "\n",
    "    figure = plt.figure(figsize=(15, 6))\n",
    "\n",
    "    ax = figure.add_subplot(121)\n",
    "    ax.scatter(x, y, alpha=0.5)\n",
    "    ax.errorbar(x, y, xerr=sigma_x, yerr=sigma_y, alpha=0.3, ls='')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "    # True regression line\n",
    "    x0 = np.linspace(-10, 10, 40)\n",
    "    y0 = alpha_in + x0 * beta_in\n",
    "\n",
    "    ax.plot(x0, y0, color='black', label='True')\n",
    "    ax.set_xlim(-12, 12)\n",
    "    ax.set_ylim(-5, 7)\n",
    "\n",
    "    ax.legend()\n",
    "    \n",
    "    \n",
    "def plot_trace(fitted, observed, ax=None, chains=None, multidim_ind=None):\n",
    "\n",
    "    traces = [fitted.trace, ]\n",
    "    xi, yi, sigx, sigy = observed\n",
    "\n",
    "    if multidim_ind is not None:\n",
    "        xi = xi[multidim_ind]\n",
    "\n",
    "    x = np.linspace(np.min(xi)-0.5, np.max(xi)+0.5, 50)\n",
    "\n",
    "    for i, trace in enumerate(traces):\n",
    "        if 'theta' in trace.varnames and 'slope' not in trace.varnames:\n",
    "            trace.add_values({'slope': np.tan(trace['theta'])})\n",
    "\n",
    "        if multidim_ind is not None:\n",
    "            trace_slope = trace['slope'][:, multidim_ind]\n",
    "        else:\n",
    "            trace_slope = trace['slope'][:, 0]\n",
    "\n",
    "        if chains is not None:\n",
    "            for chain in range(100, len(trace) * trace.nchains, chains):\n",
    "                y = trace['inter'][chain] + trace_slope[chain] * x\n",
    "                ax.plot(x, y, alpha=0.03, c='red')\n",
    "\n",
    "        # plot the best-fit line only\n",
    "        H2D, bins1, bins2 = np.histogram2d(trace_slope,\n",
    "                                           trace['inter'], bins=50)\n",
    "\n",
    "        w = np.where(H2D == H2D.max())\n",
    "\n",
    "        # choose the maximum posterior slope and intercept\n",
    "        slope_best = bins1[w[0][0]]\n",
    "        intercept_best = bins2[w[1][0]]\n",
    "\n",
    "        print(\"beta:\", slope_best, \"alpha:\", intercept_best)\n",
    "        y = intercept_best + slope_best * x\n",
    "        \n",
    "        #y_pre = fitted.predict(x[:, None])\n",
    "        ax.plot(x, y, ':', label='fitted')\n",
    "        \n",
    "        ax.legend()\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from astroML.linear_model import TLS_logL\n",
    "\n",
    "\n",
    "# TLS:\n",
    "def get_m_b(beta):\n",
    "    b = np.dot(beta, beta) / beta[1]\n",
    "    m = -beta[0] / beta[1]\n",
    "    return m, b\n",
    "\n",
    "\n",
    "def plot_figure(ksi, eta, x, y, sigma_x, sigma_y, add_regression_lines=False,\n",
    "                alpha_in=1, beta_in=0.5, basis='linear'):\n",
    "    # reproduce parts of figure 3.\n",
    "\n",
    "    # True regression line\n",
    "    x0 = np.arange(np.min(ksi) - 0.5, np.max(ksi) + 0.5)\n",
    "    \n",
    "    # TODO: do properly with .predict()        \n",
    "    if basis == 'linear':\n",
    "        y0 = alpha_in + x0 * beta_in\n",
    "    elif basis == 'poly':\n",
    "        y0 = alpha_in + beta_in[0] * x0 + beta_in[1] * x0 * x0 + beta_in[2] * x0 * x0 * x0\n",
    "           \n",
    "    figure = plt.figure(figsize=(15, 6))\n",
    "    #ax = figure.add_subplot(121)\n",
    "    #ax.scatter(ksi, eta)\n",
    "    #ax.set_xlabel(r'$\\xi$')\n",
    "    #ax.set_ylabel(r'$\\eta$')\n",
    "\n",
    "    #ax.plot(x0, y0, color='orange')\n",
    "    #ax.set_xlim(-4, 4)\n",
    "    #ax.set_ylim(-3, 3)\n",
    "\n",
    "    ax = figure.add_subplot(122)\n",
    "    ax.scatter(x, y, alpha=0.5)\n",
    "    ax.errorbar(x, y, xerr=sigma_x, yerr=sigma_y, alpha=0.3, ls='')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "        \n",
    "    # Redo truth for second panel\n",
    "    x0 = np.linspace(-10, 10, 40)\n",
    "    # TODO: do properly with .predict()        \n",
    "    if basis == 'linear':\n",
    "        y0 = alpha_in + x0 * beta_in\n",
    "    elif basis == 'poly':\n",
    "        y0 = alpha_in + beta_in[0] * x0 + beta_in[1] * x0 * x0 + beta_in[2] * x0 * x0 * x0\n",
    "\n",
    "    ax.plot(x0, y0, color='black', label='True')\n",
    "    ax.set_xlim(-12, 12)\n",
    "    ax.set_ylim(-5, 7)\n",
    "    #ax.plot([-4, 4, 4, -4, -4], [-3, -3, 3, 3, -3], color='k', alpha=0.5)\n",
    "\n",
    "    if add_regression_lines:\n",
    "        x0 = np.arange(-10, 10)\n",
    "        y0 = np.arange(-4, 6)\n",
    "        for label, data, *target in [['no err', x, y, 1],\n",
    "                                     ['y err', x, y, sigma_y],\n",
    "                                     ['x err', y, x, sigma_x]]:\n",
    "            linreg = LinearRegression()\n",
    "            linreg.fit(data[:, None], *target)\n",
    "            if label == 'x err':\n",
    "                x_fit = linreg.predict(y0[:, None])\n",
    "                ax.plot(x_fit, y0, label=label)\n",
    "            else:\n",
    "                y_fit = linreg.predict(x0[:, None])\n",
    "                ax.plot(x0, y_fit, label=label)\n",
    "\n",
    "        # TLS\n",
    "        X = np.vstack((x, y)).T\n",
    "        dX = np.zeros((len(x), 2, 2))\n",
    "        dX[:, 0, 0] = sigma_x\n",
    "        dX[:, 1, 1] = sigma_y\n",
    "\n",
    "        min_func = lambda beta: -TLS_logL(beta, X, dX)\n",
    "        beta_fit = optimize.fmin(min_func, x0=[-1, 1])\n",
    "        m_fit, b_fit = get_m_b(beta_fit)\n",
    "        x_fit = np.linspace(-10, 10, 20)\n",
    "\n",
    "        ax.plot(x_fit, m_fit * x_fit + b_fit, label='TLS')\n",
    "\n",
    "    ax.legend()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure(ksi, eta, xi[0], yi, xi_error[0], yi_error, add_regression_lines=True, alpha_in=alpha_in, beta_in=beta_in)\n",
    "plot_trace(linreg_xy_err, (xi, yi, xi_error, yi_error), ax=plt.gca(), chains=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate regression\n",
    "For multivariate data (where we fit a hyperplane rather than a straight line) we simply extend the description of the regression function to multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksi3, eta3, xi3, yi3, xi_error3, yi_error3, alpha_in3, beta_in3 = simulation_kelly(size=100, scalex=0.2, scaley=0.2,\n",
    "                                                                                   alpha=2, beta=np.array((0.5, 1, 1)),\n",
    "                                                                                   multidim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_xy_err3 = LinearRegressionwithErrors()\n",
    "linreg_xy_err3.fit(xi3, yi3, yi_error3, xi_error3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for i in range(linreg_xy_err3.trace['slope'].shape[1]):\n",
    "\n",
    "    joinpl = sns.jointplot(linreg_xy_err3.trace['slope'][:, i], linreg_xy_err3.trace['inter'], kind='kde')\n",
    "    joinpl.ax_joint.plot(beta_in3[i], alpha_in3, 'x', color='red', ms=10)\n",
    "    joinpl.ax_marg_y.plot([0, 2], [alpha_in3, alpha_in3], color='red')\n",
    "    joinpl.ax_marg_x.plot([beta_in3[i], beta_in3[i]], [0, 2], color='red')\n",
    "    \n",
    "    plot_figure(ksi3[i], eta3, xi3[i], yi3, xi_error3[i], yi_error3, add_regression_lines=False, alpha_in=alpha_in3, beta_in=beta_in3[i])\n",
    "    plot_trace(linreg_xy_err3, (xi3, yi3, xi_error3, yi_error3), ax=plt.gca(), chains=50, multidim_ind=i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
